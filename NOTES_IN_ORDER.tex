\documentclass[11pt]{report}
\usepackage[a4paper]{geometry}
\usepackage{outline}
\usepackage{pmgraph}
\usepackage[normalem]{ulem}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{eucal}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{tikz}
\begin{document}
%--------------------------------------------------------------------

\chapter{INTRODUCTION}
\section{Two thesis}
\textbf{Hyperlink} structure forms a massive directed graph - nodes in the graph represent web pages and links represent the hyperlinks, \textbf{inlinks} point into nodes, \textbf{outlinks} point out from nodes
\subsection{PageRank}
Page with more recommendations must be more important than a page with a few inlinks, status of recommender also important. \textit{PageRank's thesis is that a webpage is important if it is pointed to by other important pages.}
\subsection{HITS}
\begin{itemize}
\item Uses both inlinks and outlinks to create two popularity scores for each page
\item Hub if contains many outlinks
\item Authority if it has many inlinks
\item \textit{A page is a good hub(and therefore deserves a high hub score) if it points to good authorities, and a page is a good authority if its pointed to by good hubs.}
\end{itemize}
\section{Query-Independence}
\begin{itemize}
\item Query-independent if the popularity score for each page is determined off-line, remains constant until next query
\item At query time, no time is spent computing popularity scores for relevant pages
\item PageRank - query-independent
\item HITS - originally query-dependent
\end{itemize}
PageRank importance scores are actually the stationary values of an enormous Markov chain
\section{The Original Summation Formula for PageRank}
\begin{itemize}
\item \begin{equation}
r(P_i) = \displaystyle \sum_{P_j\in B_{P_i }} \frac{r(P_j)}{|P_j|} 
\end{equation} where $B_{P_i }$ is the set of pages pointing into $P_i$, and $\vert P_j \vert $ is the number of outlinks from the page 
\item Outlink ranks are unknown 
\item Use iterative procedure
\begin{itemize}
\item Assume all pages have equal PageRank $\frac{1}{n}$ 
\begin{equation}
r_{(k+1)}P_i = \displaystyle \sum_{P_j\in B_{P_i }}\frac{r_k(P_j)}{|P_j|}
\end{equation}
\item Initiated with \(r_0(P_i) = \frac{1}{n}\)
\end{itemize}
\end{itemize}
\section{Introduction}
\begin{itemize}
\item Assesses the importance of web pages without human evaluation of the content
\item Asks the web itself to rank the importance of pages
\end{itemize}
%--------------------------------------------------------------------
\section{How to tell who's important}
\begin{itemize}
\item The importance of a page is judged by the number of pages linking to it as well as their importance.
\item Assign to each web page \textit{P} a measure of its importance \textit{I(P)}, called the PageRank
\item Suppose a page $P_j$ has $l_j$ links, if one of those links are to page $P_i$, then $P_j$ will pass on $\frac{1}{l_j}$ of its importance to $P_i$
\item The importance ranking of $P_i$ is then the sum of all contributions made by pages linking to it
\begin{equation}
I(P_i) = \displaystyle \sum_{P_j\in B_{i}} \frac{I(P_j)}{l_j}
\end{equation}
\end{itemize}
\section{Introduction}
\begin{itemize}
\item Techniques tend to more algebraic, although graph theory and discrete probability theory appear
\item Modern search engines exploit the graph structure of the web
\item Link analysis
\item References
\begin{itemize}
\item \textit{Modelling the Internet and the Web, Probabilistic Methods and Algorithms}, Baldi, Frasconi, Smyth
\item \textit{Mining the Web, Discovering Knowledge from Hypertext Data}, Chakrabarti
\item LANGVILLE
\end{itemize}
\end{itemize}
%--------------------------------------------------------------------
\section{An Overview of Search Engines}
\begin{itemize}
\item\textit{Search Engine} defined as an information retrieval system for the web, which when queried on some topic returns a list of document which share some content or relationship to the topic
\item Web search engine possesses three components:
\begin{enumerate}
\item Crawler
\item Indexer
\item Query Engine - ranking algorithm
\end{enumerate}
\item Ranking often achieved by the assignment of a weight, or weights, to a given page, and basing the ranking on the order of these weights
\item Google's website : '... while we have dozens of engineers working to improve every aspect of Google on a daily basic, PageRank continues to provide the basis for all of our web search tools.'
\end{itemize}
\section{PageRank}
\begin{itemize}
\item Googles founders Larry Page and Sergey Brin in 1998 -\textbf{Reference: \textit{Anatomy of a large-scale hyper textual web search engine}, Brin, Page}
\item Around the same time, HITS - based on link analysis
\item PageRank is simply the stationary distribution of a random walk on a digraph
\end{itemize}
%--------------------------------------------------------------------
\chapter{MATRIX}
\section{Matrix representation}
\begin{itemize}
\item Replace sum, at each iteration compute a PageRank vector which holds PageRank values for all pages in the index
\item Introduce a \textit{n} x \textit{n} matrix \textbf{H} and a 1 x \textit{n} row vector $\boldsymbol{\pi}^T$
\item \textbf{H} row normalized hyperlink matrix with $\boldsymbol{H_{ij}} = \begin{cases} \frac{1}{P_j} &  \textnormal{if there is a link from node i to j}  \\ 0 & otherwise\end{cases}$ 
\item \textbf{H} non-zero elements are probabilities
\item Write the summation formula as \begin{equation}
\boldsymbol{\pi}^{(k+1)T} = \boldsymbol{\pi}^{(k)T}\textbf{H}
\end{equation}
\item Matrix reveals some immediate observations:
\begin{enumerate}
\item Each iteration involves one vector-matrix multiplication
\item Sparse matrix, most webpages only link to average 10 other pages - require minimal storage, requires \textit{O(nnz}\textbf{H}), linear
\item Classical power method applied to \textbf{H}
\item \textbf{H} looks like a stochastic transition probabilty matrix for a Markov chain
\end{enumerate}
\end{itemize}
\section{Problems with the Iterative Process}
\begin{itemize}
\item Rank sinks, pages that accumulate PageRank with each iteration - dangling nodes and cycles
\item Prefer PageRank vector to be positive
\end{itemize}
\section{Markov theory}
\begin{itemize}
\item Power method applied to a Markov chain with transition probabilty matrix \textbf{H}
\item Know for any starting vector, the power method on a matrix converges to a stationary vector as long as the matrix is stochastic, irreducible and aperiodic (aperiodicity and irreducibility implies primitvity)
\end{itemize}
\section{Early Adjustments}
\begin{itemize}
\item Grand application of Markov chains
\item Random surfer idea
\item \textbf{Stochasticity adjustment} - where rows of 0â€™s are changed to rows of 1/n. In terms of the random surfer, this means that if it enters a dangling node it can jump to another page at random 
\item \begin{equation}
\textbf{S} = \textbf{H} + \textbf{a}(\frac{1}{n}\cdot 1^{T}
\end{equation}
\item \textbf{a} is the dangling node vector, \textbf{S} is the stochastic matrix
\item Guarantees that S is stochastic, and is the transition probabilty matrix for a Markov chain - but not convergence
\item \textbf{Primitivity adjustment} - random surfer follows \textbf{S} with probability \(\alpha\), but can jump to a random page with probability \(1-\alpha\), forming the Google Matrix \textbf{G}
\item \begin{equation}
\textbf{G} = \alpha\textbf{S} + (1-\alpha)\cdot\frac{1}{n}\cdot 1^{T}
\end{equation}
\item Teleporting is random, equally likely to jump to any page
\item Several consequences of the primitivity adjustment
\begin{enumerate}
\item \textbf{G} is stochastic, combination of two stochastic matrices
\item \textbf{G} is irreducible
\item \textbf{G} is aperiodic
\item \textbf{G} is primitive, because \(\textbf{G}^k>0\) for some k, implies that a unique positive \(\boldsymbol{\pi}^T\) exists, and the power method applied to \textbf{G} is guaranteed to converge to this vector
\item \textbf{G} is dense, bad thing computationally, can be written in terms of the sparse \textbf{H}
\begin{equation}
\begin{split}
\textbf{G} &= \alpha\textbf{S} + (1-\alpha)\cdot\frac{1}{n}\cdot 1^{T}\\
&= \alpha(\textbf{H} +\frac{1}{n}\textbf{a}\cdot1^{T}) + (1-\alpha)\cdot\frac{1}{n}\cdot1^T \\
&= \alpha\textbf{H} + (\alpha\textbf{a}+(1-\alpha))\frac{1}{n}\cdot1^T
\end{split}
\end{equation}
\item \textbf{G} is artificial, modified twice in order to produce desirable convergence properties
\end{enumerate}
\item Google's adjusted PageRank method is the power method applied to \textbf{G} \begin{equation}
\boldsymbol{\pi}^{(k+1)T} = \boldsymbol{\pi}^{(k)T}\textbf{G}
\end{equation}
\end{itemize}
\section {The $\alpha$ factor}
\begin{itemize}
\item $\alpha$ controls the priority given to the Web's natural hyperlink structure
\item 0.85 is workable compromise between efficiency and effectiveness
\item Affects sensitivity of resulting PageRank vector
\item As $\alpha\rightarrow 1$, artificiality reduces, but computation time increases
\item As $\alpha\rightarrow 1$ PageRankings become more volatile
\end{itemize}
\begin{itemize}
\item Create the Hyperlink matrix
\begin{equation}
\boldsymbol{H_{ij}} = \begin{cases} \frac{1}{l_j} & P_j\in B_{i} \\ 0 & otherwise\end{cases}
\end{equation}
\item H is stochastic - all entries are non-negative and the sum of the entries in every column is one
\item Form a vector $I=[I(P_i)]$ whose components are PageRanks of all pages \begin{equation}
I=\textbf{H}I 
\end{equation} In other words, the vector I is an eigenvector of the matrix \textbf{H} with eigenvalue 1, stationary vector of \textbf{H}
\end{itemize}
\section{A Probabilistic interpretation of H}
\begin{itemize}
\item Can interpret a web pages PageRank as the fraction of time that a random surfer spends on that web page
\item More likely to come back to more important web pages
\item If we surf randomly, can get stuck at dangling nodes, pretend that a dangling node has a link to every page 
\item Effectively replacing a column of zeros corresponding to a dangling node with a column in which each entry is $\frac{1}{n}$ - Stochastic Matrix \textbf{S}
\item \textbf{S} stochastic- non-negative entries, and the sum of the entries in each column is one, stochastic matrices always have stationary vectors
\item If \textbf{A} is the matrix whose entries are al zero except for the columns relating to dangling nodes, where the value is $\frac{1}{n}$ then, \textbf{S}=\textbf{H}+\textbf{A}
\end{itemize}
\section{When things go wrong}
\begin{itemize}
\item Assumed prior that \textbf{S} had property $\lambda_1 = 1$ and $\vert\lambda_2\vert<1$, but not always this case
\item $I^k$ can fail to converge
\item To guarantee $\vert\lambda_2\vert<1$ need \textbf{S} to be primitive, all positive entries
\item Need to be irreducible, guarantee there is a stationary vector with all positive entries
\end{itemize}
\section{A final modification}
\begin{itemize}
\item First choose a parameter $\alpha\in[0,1]$ 
\item With probability $\alpha$, guided by \textbf{S}, probabilty $1-\alpha$ chooses next page at random
\item Google Matrix \begin{equation}
\textbf{G}=\alpha\textbf{S} +(1-\alpha)\frac{1}{n}1
\end{equation}
\item \textbf{G} is stochastic, primitive and irreducible
\item If $\alpha=1$ the \textbf{G}=\textbf{S} - working with original hyperlink structure of the web 
\item If $\alpha=0$ then $\textbf{G}=\frac{1}{n}1$ - links between every two pages, lost the hyperlink structure
\item Want $\alpha$ close to 1 so hyperlink structure is weighted heavily in the computation
\item For Google matrix, proven that $\vert\lambda_2\vert = \alpha$, means when $\alpha$ close to 1, then slow convergence
\item $\alpha= 0.85$
\end{itemize}

\begin{itemize}
\item Random surfer, how to model?
\begin{itemize}
\item Most of the time surfer will follow links from a given page \textit{i}, the surfer will move to one of the out-neighbours of \textit{i}. 
\item The surfer could also choose a page uniformly at random from the graph and 'teleport' there
\end{itemize}
\item Random process is a Markovian one, which converges to a stationary distribution.
\item Given G = W and a fixed but arbitrary enumeration of V(G), form the transition matrix $P_1$ for the random walk on G:
\begin{equation}
P_1(i,j) = \begin{cases} \frac{1}{deg^+(i)} & if (i,j) \in E(G) \\ 0 & otherwise\end{cases}
\end{equation}
\item Problem that the out-degree for a vertex \textit{u} might be 0, and so then stuck
\item Overcome this, fix a constant \(c \in (0,1] \) - choice of 0.15 - REFERENCE BRIN AND PAGE
\item Form $P_2$ by replacing any zero rows of $P_1$ by the vector \(\frac{1}{nJ_{n,1}}\) where \(n=|V(G)|\)
\item Note this matrix is non-negative, and stochastic hence is the transition matrix for a Markov Chain.
\item The corresponding Markov chain of P(W) may be viewed as a certain random walk on vertices of W; we call these the PageRank Markov chain or PageRank random walk.
\item At a vertex u, with probabilty c, move to an out-neighbour u.a.r (Uniformly at random) (if no neighbours then move to any vertex W u.a.r).
\item With probabilty (1-c) move to any vertex in W chosen u.a.r
\item The constant c is the teleportation constant (or damping factor) : measures how often a surfer teleports to a new page rather than following links.
\item The vector \textbf{s} is the PageRank vector and the \textit{i}'th entry is the PageRank of the \textit{i}'th page of G.
\end{itemize}
\section{Mining the Web}
\begin{itemize}
\item An analysis of a significant portion of the Web graph in 2000 showed it is not strongly connected as a whole, only a fourth is
\end{itemize}
%--------------------------------------------------------------------
\chapter{SOLVING}
\section{Computation of PageRank vector}
\begin{itemize}
\item Problem can be stated in two ways
\begin{enumerate}
\item Solve the following eigenvector problem for $\pi^T$ \begin{eqnarray}
\boldsymbol{\pi}^T = \boldsymbol{\pi}^T\textbf{G}\\
\boldsymbol{\pi}^T\textbf{e}=1
\end{eqnarray}
\item Solve the following linear homogeneous system for $\pi^T$
\begin{eqnarray}
\boldsymbol{\pi}^T(\textbf{I}-\textbf{G})=\textbf{0}^T\\
\boldsymbol{\pi}^T\textbf{e}=1
\end{eqnarray}
\end{enumerate}
\item In 1. goal is to find a normalized dominant left-hand eigenvector of \textbf{G} corresponding to the dominant eigenvalue $\lambda_1 = 1$
\item In 2. want to find normalized left-hand null vector of \textbf{I} - \textbf{G}
\item Power method
\begin{itemize}
\item Slow
\item Simple
\item Vector-matrix multiplication can be done on sparse \textbf{H}, \textbf{S} and \textbf{H} are never formed or stored, only rank one components \textbf{a} and \textbf{e} are needed
\item Matrix-free
\item Storage friendly, sparse matrix, dangling node vector and current iterate need to be stored, n real numbers - GMRES(10) require 10n
\item 50-100 iterations needed - 2/3 degrees of accuracy
\end{itemize}
\end{itemize}

\section{Computing I}
\begin{itemize}
\item Most of the entries in \textbf{H} are zero, web pages have on average 10 out-links
\item Power method for finding the stationary vector
\begin{itemize}
\item Begin by choosing a vector $I^0$ as a candidate for I and then producing a sequence of vectors $I^k$ by $I^{k+1} = HI^k$
\item The sequence will converge to the stationary vector
\item No absolute measure of a pages importance
\end{itemize}
\end{itemize}
\section{How does the power method work?}
\begin{itemize}
\item Looking for an eigenvector of \textbf{S} corresponding to the eigenvalue 1
\item Other eigenvalues of \textbf{S} will have magnitude smaller than one; that is $\vert\lambda\vert <1$ if $\lambda$ is an eigenvalue of \textbf{S} other than 1
\item We will assume that the eigenvalues of \textbf{S} are $\lambda_j$ and that \begin{equation}
1=\lambda_1>\vert\lambda_2\vert\geq\vert\lambda_3\vert\geq\ldots\geq\vert\lambda_n\vert
\end{equation}
\item Assume there is a basis $v_j$ of eigenvectors for \textbf{S} with corresponding eigenvalues $\lambda_j$ 
\item Write initial vector $I_0$ as \begin{equation}
I^0 = c_1v_1 + c_2v_2 + \ldots + c_nv_n
\end{equation}
then \begin{multline}
I^1 =\textbf{S}I^0= c_1v_1 + c_2\lambda_2v_2 + \ldots + c_n\lambda_nv_n\\
I^2 =\textbf{S}I^1= c_1v_1 + c_2\lambda_2^2v_2 + \ldots + c_n\lambda_n^2v_n\\
\vdots\\
I^k =\textbf{S}I^{k-1}= c_1v_1 + c_2\lambda_2^kv_2 + \ldots + c_n\lambda_n^kv_n
\end{multline}
\item Since the eigenvalues $\lambda_j$ with $j\geq2$ have magnitude $\leq1$, follows that $\lambda^k_j\rightarrow0$ if $j\geq2$ and so $I^k\rightarrow I=c_1v_1$, an eigenvector corresponding to eigenvalue 1
\item Rate is determined by $\vert\lambda_2\vert$, when close to 0, then $\lambda_2^k \rightarrow0$ relatively quickly
\end{itemize}
%--------------------------------------------------------------------
\section{Three important questions}
\begin{itemize}
\item Does the sequence always converge?
\item Is the vector to which it converges independent of the initial vector?
\item Do the importance rankings contain the information that we want?
\item Can do some modifications so that the answer is yes to all three
\item Dangling nodes
\end{itemize}
\section{Computing I}
\begin{itemize}
\item \textbf{S}=\textbf{H}+\textbf{A}
\item Google Matrix has form \begin{equation}
\textbf{G}=\alpha\textbf{H}+\alpha\textbf{A}+\frac{1-\alpha}{n}1
\end{equation} therefore \begin{equation}
\textbf{G}I^k=\alpha\textbf{H}I^k+\alpha\textbf{A}I^k+\frac{1-\alpha}{n}1I^k
\end{equation}
\item Evaluating $\textbf{H}I^k$ requires only 10 nonzero terms for each entry in the vector
\item The rows of \textbf{A} are all identical as the rows of \textbf{1}
\item Evaluating $\textbf{A}I^k$ and $\textbf{1}I^k$ amounts to adding the current importance rankings of all the dangling nodes or all the web pages
\item 50-100 iterations required to obtain sufficiently good approximation to I
\item Calculation takes a few days to complete
\item Google recomputes PageRank vector I roughly every months - Google Dance
\end{itemize}
\begin{itemize}
\item How to compute the PageRank vector?
\begin{itemize}
\item Since \begin{equation} \textbf{s}^T P = \textbf{s}^T\end{equation} we must solve the homogenous linear system \begin{equation}(I_n-P)\textbf{s}^T=0\end{equation} making sure to normalize so that \(||s||_\infty =1\)
\end{itemize}
\item Huge computational issues arise when we recall W is a massive graph, n about 54 billion
\item Power method
\begin{itemize}
\item Algorithm initializes with a vector $\textbf{z}_0$
\item Choose $\textbf{z}_0$ to be the stochastic vector whose every entry is $\frac{1}{n}$ where \(n=|V(G)|\)
\item Assume $\textbf{z}_t$ is defined and stochastic
\begin{equation}
\textbf{z}_{t+1}^T = \textbf{z}_{t}^TP = \textbf{z}_{0}^TP^t
\end{equation}
\end{itemize}
\item Power method slow to converge, it is useful for PageRank as $P_1$ is sparse
\item Let \textbf{u} be the row-vector defined by \begin{equation} u_i =\begin{cases} 0& if deg^+(i) >0 \\ 1 & else\end{cases}\end{equation}
\item It follows that \begin{equation}P=cP_1 +(c\textbf{u} +(1-c)J_{n,1})\frac{1}{n}J_{1,n}\end{equation}
\item And so \begin{equation}
\textbf{z}^T_{(t+1)} = \textbf{z}_t^TP=c\textbf{z}^T_tP_1 +(c\textbf{z}^T_t\textbf{u} +(1-c)\textbf{z}^T_tJ_{n,1})\frac{1}{n}J{1,n}
\end{equation}
\item As $P_1$ sparse, the matrix product of $c\textbf{z}^T_tP_1$ is easier to compute
\item Brin and Page reported good results between 50-100 iterations
\item other numerical methods such as Gauss-Seidel and Jacobi method - \textbf{See chapter 9.4 of LANGVILLE}
\end{itemize}

%--------------------------------------------------------------------
\chapter{IMPROVEMENTS}
\section{The Hyperlink matrix \textbf{H}}
\begin{itemize}
\item Random surfer may not be accurate at all - equality not best for webpages
\item Replace with \textbf{intelligent surfer}
\item May be more likely to jump to a more content-filled pages, so these pages should be given more probabilistic weight
\item Practical way to do this - use access logs to find actual surfer tendencies, and then adjust probabilities in \textbf{H} accordingly
\item Other methods to fill elements in \textbf{H} - REFERENCES \textbf{Baeza-Yates,Davis} \textit{Web page ranking using link attributes}, \textbf{Bharat, Henzinger} \textit{Improved algorithms for topic distillation}, \textbf{Bharat, Maghoul, Stata}, \textit{The term vector database}, \textbf{Richardson, Domingos}, \textit{The intelligent surfer;Probablistic combination of link and content information in PageRank}, \textbf{Tomlin}, \textit{A new paradigm for ranking pages on the WWW}
\item These methods use heuristic (hands-on) rules to create the nonzero elements of \textbf{H} by combining location of outlinks, length of anchor text, and content similarity
\end{itemize}
\section{The Teleportation Matrix E}
\begin{itemize}
\item Rather than use $\frac{1}{n}1^T$, used $\textbf{ev}^T$ where $\textbf{v}^T >0$ is the personalization vector
\item Using personalization vector, means that the probabilties are no longer uniformly distributed
\item Retains the advantageous properties of power method
\item When $\textbf{G} = \alpha\textbf{S}+(1-\alpha)\textbf{ev}^T$ \begin{equation}
\begin{split}
\boldsymbol{\pi}^{(k+1)T} &=\boldsymbol{\pi}^{(k)T}\textbf{G}\\
&=\alpha\boldsymbol{\pi}^{(k)T}\textbf{S} + (1-\alpha)\boldsymbol{\pi}^{(k)T}\textbf{ev}^T\\
&=\alpha\boldsymbol{\pi}^{(k)T}\textbf{H}+(\alpha\boldsymbol{\pi}^{(k)T}\textbf{a}+1-\alpha)\textbf{v}^T
\end{split}
\end{equation}
\item Only thing that changes is the PageRank vector itself, different personalizations produce different PageRankings REFERENCE: \textbf{Thorson}, \textit{Modeling the web and the computation of PageRank}
\item $\boldsymbol{\pi}^T(\textbf{v}^T)$ is a function of $\textbf{v}^T$
\item Original says nothing about me and my preferences
\item In practice is computationally impossible
\item Pseudo-personalized PageRanking systems, REFERENCE: \textbf{Haveliwala}, \textit{Efficient computation of PageRank}
\item Haveliwala - topic sensitive Page Rank
\begin{itemize}
\item Finite number of PageRank vectors each biased towards a particular topic
\item 16 top-level topics - Arts, Business etc.
\item Biased PageRank vectors calculated ahead of time
\item Topic-sensitive, query-dependant PageRank vector as a convex combination of PageRank vectors
\item Uses a Bayesian classifier to compute the $\beta_i$'s for the experiments
\end{itemize}
\item Personalization vector could be used to control spamming by link farms
\end{itemize}
\begin{itemize}
\item Form $P_2$ by replacing any zero rows of $P_1$ by the vector \(\frac{1}{nJ_{n,1}}\) where \(n=|V(G)|\)
\item Personalization vector (arbitrary stochastic vector) can replace the vector \(\frac{1}{nJ_{n,1}}\)
\item Define the PageRank (Google) Matrix by \[P = P(G) = cP_2 +\frac{1-c}{n}J_{n,n} \] where $J_{n,n}$ is the [\textit{n} x \textit{n}] matrix of all 1's.
\item Vulnerable to spamdexing - deliberate spamming attempts to influence Google's ranking of a page

\end{itemize}
\section{Further Analysis of Web Ranking Algorithms}
\begin{itemize}
\item Spamdexing
\begin{itemize}
\item Intentional attempt to improve the ranking of a page on a search engine 
\item Link spam
\item As PageRank is sensitive to in-degree, a simple way to spamdex PageRank is to increase the in-degree to your page
\item Link farm is a way to do this
\end{itemize}
\item Analysis and comparison of ranking algorithms
\begin{itemize}
\item If matrices with HITS are not primitive, then the eigenvectors need not to be unique: \textbf{Reference: Farahat, LoFaro, Miller, Rae, Ward: Authority rankings from HITS, PageRank, and SALSA: existence, uniqueness, and effect of initialization}
\item PageRank matrix is always primitive
\item A ranking algorithm that assigns hub and authority weights is badly behaved on a graph G if the final result depends on the initial vectors or if some vertex is given authority (hub) weight zero even though it has in-degree (out-degree) greater than zero
\end{itemize}
\item Stability
\begin{itemize}
\item Measures the effect on rankings if the graph is perturbed: edges are added or deleted
\item HITS is insensitive to minor changes
\item If a set of pages in V' have low PageRank, then the change in PageRank will be small
\end{itemize}
\end{itemize}
\section{Topic-specific PageRank}
\begin{itemize}
\item Teleporting to a page chosen non uniformly
\item Derive PageRank values tailored to interests
\item Not focus on how to collect all web pages on a topic - e.g. sports, need a nonzero subset of sorts-related web pages
\item Provided the set of sports-related pages is non-empty set of web $Y\supseteq S$ over which the random walk has a steady-state distribution
\item This \textit{sports PageRank} distribution is noted as $\vec{\pi}_s$
\item For web pages not in Y, set PageRank value to 0
\item Can do this for other topics, e.g. politics, religion ... 
\item Potential of considering settings in where the search engine knows what topics the user is interested in - users explicitly state topics, or system learns over time\\
\includegraphics[width = 10cm]{Topic-specific_PageRank_Manning.png}
\item User have a mixture of interests - e.g. interest mixture 60\% and 40\% politics
\item Find a personalized PageRank for a user
\item Assume that an individual's interest can we well-approximated as a linear combination of a small number of topic page distributions
\item Leads to a Markov chain with a steady state distribution that is personalized
\item Implementation is cumbersome, demand that for each user need compute a transition matrix and steady-state distribution
\item Personalized PageRank vector can be expressed as a linear combination of underlying topic-specific PageRanks
\item E.g. Personalized PageRank $\vec{\pi} = 0.6\vec{\pi}_s + 0.4\vec{\pi}_p$ 
\end{itemize}
\section{Web page ranking using link attributes}
\begin{itemize}
\item WLRank - Weighted Links Rank, considers different web page attributes to give more weight to some links
\item Gives weights to link based on three attributes, relative position in the thread, tag where the link is contained and length of the anchor text
\item Gives more weight to links that are at beginning of pages
\end{itemize}
\section{The Term Vector Database: fast access to indexing terms for Web pages}
\begin{itemize}
\item Built term vector database
\end{itemize}
\section{The Intelligent Surfer:
Probabilistic Combination of Link and
Content Information in PageRank}
\begin{itemize}
\item Improve PageRank by using a more intelligent surfer - guided by a probabilistic model of the relevance of a page to a query
\item Algorithm outperforms PageRank in human rated quality of pages returned, remianing efficient enough to be used in large SE
\item 'PageRank rates a page highly if it is at the center of a large sub-web'
\item However best pages should be those at the centre but also relevant
\item Does most of computation at crawl time
\item Algorithm closer to what a human would do, jumps preferentially to pages containing query terms
\item Can be viewed as solution to problem of topic drift
\item Resulting probability distribution over pages is \begin{equation}
P_q(j)=(1-\alpha)P_q'(j)+\alpha\sum_{i\in B_j}P_q(i)P_q(i\rightarrow j)
\end{equation} where $P_q(i\rightarrow j)$ is the probability that the surfer transitions to page \textit{j} given that he is on page \textit{i} and searching for query \textit{q}, $P_q'(j)$ specifies where the surfer chooses to jump when not following links, $P_q(j)$ is the resulting probability distribution over pages and corresponds to the query-dependent PageRank score
\item When choosing among multiple out-links, the directed surfer tends to follow those which lead to pages whose content has been deemed relevant to the query
\item SE cannot perform computation at query time, when expected to return results in seconds
\item Pre computing individual term rankings and combing at query time, computation and storage required for QD PageRank, hundred of thousands of words is only approximately 100-200 times that of normal PageRank
\end{itemize}
\section{A new paradigm for ranking pages on the world wide web}
\begin{itemize}
\item Using a method of entropy maximisation
\item Constructed on a large scale 
\end{itemize}
\section{Efficient Computation of PageRank}
\begin{itemize}
\item Applying biases during iterations to increase the importance of certain categories and pages
\item Efficient version of algorithm which lowers main memory requirements
\item As few as 10 iterations required
\item Useful PageRank vector in roughly an hour
\end{itemize}
\section{Topic Sensitive PageRank}
\begin{itemize}
\item Set of PageRank vectors biased using a set of representative topics
\item Generate more accurate rankings than with a single, generic PageRank vector
\item Like HITS allows the query to influence the link-based score, but requires minimal query-time processing like PageRank
\item Personalization
\item Biasing involves introducing artificial links into the Web graph during the off-line rank computation
\item By making PageRank topic-sensitive, we avoid the problem of heavily linked pages getting highly ranked for queries where they have no particular authority
\item Small number of representative basis topics
\item Context can come from user who submitted query - bookmarks and browser history
\item During offline crawl, 16 topic-sensitive PageRank vectors 
\item At query time, calculate similarity of query (and if available, the query or user context) to each of these topics
\item Take linear combination of topic-sensitive vectors weighted using the similarities of the query (and context) to the topic
\item Link-based computations are performed offline, the query-time costs are not much greater
\item Topics - Arts, Business, Computers, Games, Health, Home, Kids \& Teens, News, Recreation, Reference, Regional, Science, Shopping, Society, Sports, World
\end{itemize}
\section{Modelling the Web and the computation of PageRank}
\begin{itemize}
\item Perron-Frobenius theorem that a positive, stochastic, irreducible matrix is guaranteed to have a positive eigenvector
\item Weakness of PageRank - web contains so many documents that calculations must be completed on many computers
\item Structure and content of the web changes daily - monthly updating though
\item Personalized PageRank costly in implementation, allowed bias against link farms
\item Pages with lover PageRank tend to converge very quickly compared to pages with higher PageRank
\end{itemize}
%--------------------------------------------------------------------
\chapter{HITS\&SALSA\&COMPARISONS}
\section{The HITS Algorithm}
\begin{itemize}
\item Hypertext Induced Topic Search - Jon Kleinberg in 1998
\item Like PageRank, uses web hyperlink to create popularity scores associated with webpages
\item HITS produces two popularity scores, query dependant
\item Thinks of web pages as authorities and hubs
\item An authority is a page with many inlinks, a hub is a page with many outlinks
\item Good authorities are pointed to by good hubs, and good hubs point to good authorities
\item Every page is some measure of an authority and some measure of a hub
\item Mathematics
\begin{itemize}
\item Every page has both an authority score $x_i$ and a hub score $y_i$
\item Let E be the set of all directed edges in the web graph and let $e_{ij}$ represent the directed edge from node \textit{i} to node \textit{j}
\item Each page is assigned initial authority score $x_i^{(0)}$ and initial hub score $y_i^{(0)}$, redefines by computing \begin{equation}
x_i^{(k)} = \displaystyle \sum_{j:e_{ij}\in E} y_j^{(k-1)} \quad\mathrm{and}\quad y_i^{(k)} = \displaystyle \sum_{j:e_{ij}\in E} x_j^{(k)}  \quad\mathrm{for}\quad k=1,2,3\ldots
\end{equation}
\item These can be written in matrix with the use of adjacency matrix \textbf{L} of the directed graph \begin{equation}
\textbf{L}_ij = \begin{cases} 1, & \textnormal{if there exists an edge from node \textit{i} to node \textit{j}}\\ 0 & \textnormal{otherwise}
\end{cases}
\end{equation}
\item In matrix notation, the equations become \begin{equation}
\textbf{x}^{(k)} = \textbf{L}^T\textbf{y}^{(k-1)}\quad\mathrm{and}\quad \textbf{y}^{(k)}=\textbf{Lx}^{(k)}
\end{equation} where \(\textbf{x}^{(k)}\) and \(\textbf{y}^{(k)}\) are \(\textit{n}x1\) vectors holding the approximate scores at each iteration
\item The Original Hits Algorithm
\begin{enumerate}
\item Initalize: \(\textbf{y}^{(0)}=\textbf{e} \) where \textbf{e} is the column vector of all ones
\item Until convergence, do \begin{eqnarray}
\textbf{x}^{(k)} =\textbf{L}^T\textbf{y}^{(k-1)}\\
\textbf{y}^{(k)} =\textbf{Lx}^{(k)}\\
k=k+1
\end{eqnarray}
\end{enumerate}
\item In step 2 of the algorithm, the equations (4.4) and (4.5) can be simplified by substitution to \begin{eqnarray}
\textbf{x}^{(k)} = \textbf{L}^T\textbf{Lx}^{(k-1)}\\ \textbf{y}^{(k)}=\textbf{LL}^T\textbf{y}^{(k-1)}
\end{eqnarray}
\item These equations define the iterative power method for computing the dominant eigenvector for the matrices $\textbf{L}^T\textbf{L}$ and $\textbf{LL}^T$
\item \textbf{Authority Matrix} is $\textbf{L}^T\textbf{L}$
\item \textbf{Hub Matrix} is $\textbf{LL}^T$
\item These matrices are symmetric
\end{itemize}
\end{itemize}
\section{HITS Implementation}
\begin{itemize}
\item Two main steps
\begin{enumerate}
\item Neighbourhood graph N related to query terms is built
\item Authority and hub scores for each page are computed, and two ranked lists are presented to the user
\end{enumerate}
\item Focus on the first step:
\item A simple method consults the inverted file index 
\begin{itemize}
\item For each term, the pages that mention the term are stored in list form
\item Graph around the subset of nodes in N is expanded by adding nodes that point either to or from nodes in N - usually resolves the problem of synonyms 
\item Set N can become very large, so max number of inlinking pages and outlinking pages to add for a particular node is fixed, e.g. 100
\end{itemize}
\item When N is built, adjacency matrix \textbf{L} is formed
\item Order of \textbf{L} is much smaller than number of pages on the web
\item Computing authority and hub scores is a lot less costly computationally as opposed to PageRank
\item Only one dominant  eigenvector needs to be computes, as you are able to use equations to find the other
\end{itemize}
\section{HITS convergence}
\begin{itemize}
\item Power method applied to $\textbf{L}^T\textbf{L}$ and $\textbf{LL}^T$
\item These matrices are symmetric, positive semidefinite and nonnegative
\item HITS with normalization always converges
\item Cannot give a good approximation to the asymptotic rate of convergence
\item Researchers suggest that only 10-15 iterations are required : REFERENCES: \textbf{Ding,He,Husbands}, \textit{PageRank, HITS and a unified framework for link analysis}, \textbf{Kleinberg}, \textit{Authoritative sources in a hyperlinked environment}, \textbf{Ng, Zheng, Jordan}, \textit{Link analysis, eigenvectors and stability} and \textit{Stable algorithms for link analysis}
\item Can be a problem with the uniqueness of the limiting authority and hub vectors
\item Structure of \textbf{L} and allow $\lambda_1$ to be a repeated root
\item Means that different limiting authority and hub vectors can be produced by different choices of the initial vector
\item Reducibility - Perron-Frobenius (REFERENCE: \textbf{Meyer}, \textit{Matrix Analysis and Applied Linear algebra}) ensures that an irreducible nonnegative matrix possesses a unique normalized positive dominant vector, Perron vector
\item Reducibility of $\textbf{L}^T\textbf{L}$ that causes HITS to converge to nonunique solutions, PageRank forces irreducibility
\item Modification similar to the primitivity adjustment can be applied
\item Modified authority matrix \(\xi\textbf{L}^T\textbf{L} +\frac{(1-\xi)}{n}1^T\) can be created, where $0<\xi<1$ REFERENCE:\textbf{Ng et al}, \textit{Stable algorithms for link analysis}
\end{itemize}
\section{Strengths and weaknesses of HITS}
\begin{itemize}
\item Advantage - dual ranking - nice to have the options
\item Advantage - ranking problem is small, size of matrices is small relative to the web
\item Disadvantage - query-dependant - at each query time, N must be formed and eigenvector solved - can make HITS query-independent, just remove the neighbourhood graph step
\item Disadvantage - susceptible to spamming, by adding links to and from your webpage, can influence authority and hub scores, influencing hub score not difficult, but they share a mutual dependence and so will increase together, since N is small in comparison, local changes to the link structure appear more drastically
\item Disadvantage - Topic drift, when building N, possible that a very authoritative yet off-topic page can be linked to pages containing the query terms, and can skew the results to off-topic pages - solution, weight pages by a measure of their relevancy REFERENCE:\textbf{Bharat, Henzinger}, \textit{Improved algorithms for topic distillation in hyperlinked environments}  - solution similar to intelligent surfer
\end{itemize}
\section{SALSA}
\begin{itemize}
\item Stochastic Approach to Link Systems Analysis - Ronny Lempel and Schlomo Moran
\item Incorporates ideas from both HITS and PageRank
\item Both hub and authority scores for webpages - Like HITS
\item Derived from Markov Chains - Like PageRank
\item Method
\begin{itemize}
\item Form neighbourhood graph, N
\item Bipartite undirected graph G built
\item G defined by three sets, $V_h$,$V_a$,$E$ where $V_h$ is the set of hub nodes, $V_a$ the set of authority nodes, $E$ the set of directed edges in N, node can be in $V_h$ and $V_a$
\item Hub side and authority side
\item \textbf{H} matrix formed - SALSA's hub matrix, \textbf{A} - Authority matrix
\item SALSA uses both row and column weighting to compute hub and authority scores
\item If G is connected, the \textbf{H} and \textbf{A} are both irreducible Markov chains
\item $\boldsymbol\pi_h^T$ is the stationary vector of \textbf{H} and gives the hub scores
\item $\boldsymbol\pi_a^T$ is the stationary vector of \textbf{A} and gives authority scores
\end{itemize}
\end{itemize}
\section{Strengths and weaknesses of SALSA}
\begin{itemize}
\item SALSA not victimized by topic drift
\item Less susceptible to spamming - coupling between hub and authority score less strict, HITS and SALSA easier to span than PageRank
\item Dual Rankings
\item Computationally easier
\item Query-dependence
\end{itemize}
\section{HITS}
\begin{itemize}
\item Hyperlink-induced topic search - John Kleinburg in the late 1990's - \textbf{Reference: Kleinburg, Authoritative sources in a hyperlinked environment}
\item Topics can be divided into authorities and hubs
\item Authorities are pages with good coverage of a topic
\item Hubs are pages with links to many useful pages on a topic
\item Weights are defined recursively: a higher authority weight occurs if the page is pointed to by pages with high hub weights, while a higher hub weight results if the page is pointed to many pages with high authority weight
\item Used in Ask.com 
\item Two main components: a sampling step and a weight-propagation step
\item Sampling step
\begin{itemize}
\item Given a query \textit{q}, a focused collection of several thousand web pages is generation, which are likely to be relevant to \textit{q}
\item We call the subgraph of W induced by these set of pages \textit{S}, and name it the base digraph.
\item We will assume that the underlying graph of \textit{S} (formed by forgetting the orientation of directed graphs) is connected
\end{itemize}
\item Weight-propagation step
\begin{itemize}
\item The authority- and hub- values are computed for pages in the base digraph
\item Every page \textit{p} in \textit{S}, will be assigned non-negative real numbers which are the authority and hub weights of \textit{p}
\item Weights are calculated by computing vectors \textbf{p(a)} and \textbf{p(h)}, defined as limit vectors of a certain recursive sequence of vectors
\item The authority and hub weight of \textit{i} can be simply read off as the \textit{i}th entries in \textbf{p(a)} and \textbf{p(h)}
\item Define \(\textbf{p(a)}_0 = \textbf{p(h)}_0 = J_{1,n}\); this is sometimes called the \textit{seed vector}
\item For \(\textit{t}\geq 0\), assuming that \(\textbf{p(a)}_t\) and \(\textbf{p(h)}_t\) are defined, define \begin{equation} \textbf{p(a)}_{(t+1)}' = A(S)^T\textbf{p(h)}_t \end{equation} and \begin{equation} \textbf{p(h)}_{(t+1)}' = A(S)^T\textbf{p(a)}'_t \end{equation} Now set \begin{equation}
\textbf{p(a)}_{(t_1)} = \frac{\textbf{p(a)}_{(t+1)}'}{\Vert\textbf{p(a)}_{(t+1)}'\Vert},   \textbf{p(h)}_{(t_1)} = \frac{\textbf{p(h)}_{(t+1)}'}{\Vert\textbf{p(h)}_{(t+1)}'\Vert}
\end{equation} 
\item The sequences \begin{equation} \textbf{(p(a)}_t:t\in \mathbb{N}), \textbf{(p(h)}_t:t\in \mathbb{N})
\end{equation} converge to the dominant eigenvectors of certain matrices, and these limits are \textbf{p(a)} and \textbf{p(h)}
\end{itemize}
\item HITS is query dependent - first generates a set S of pages likely to be relvant to the query, then returns two real numbers for a given page, PageRank returns a real number for each page in W
\end{itemize}
%--------------------------------------------------------------------
\section{SALSA}
\begin{itemize}
\item Combines some features of both HITS and PageRank
\item Stochastic Approach for Link Structure Analysis by Lempel and Moran: \textbf{Reference Moran: The SALSA and the TKC effect In, proceedings of the 9th international WWW conference on Computer networks 2000}
\item Attempt to avoid certain problems as reported with HITS
\begin{itemize}
\item Topic drift - some vertices in the base digraph are not related to the query
\item Like PageRank, uses random walks
\item Like HITS, computes scores on a proper connected induced subgraph S of W
\end{itemize}
\item Constructed graphs with good authorities, where SALSA gives higher authority weights than HITS 
\item Also query dependant
\end{itemize}
\section{Further Analysis of Web Ranking Algorithms}
\begin{itemize}
\item Spamdexing
\begin{itemize}
\item Intentional attempt to improve the ranking of a page on a search engine 
\item Link spam
\item As PageRank is sensitive to in-degree, a simple way to spamdex PageRank is to increase the in-degree to your page
\item Link farm is a way to do this
\end{itemize}
\item Analysis and comparison of ranking algorithms
\begin{itemize}
\item If matrices with HITS are not primitive, then the eigenvectors need not to be unique: \textbf{Reference: Farahat, LoFaro, Miller, Rae, Ward: Authority rankings from HITS, PageRank, and SALSA: existence, uniqueness, and effect of initialization}
\item PageRank matrix is always primitive
\item A ranking algorithm that assigns hub and authority weights is badly behaved on a graph G if the final result depends on the initial vectors or if some vertex is given authority (hub) weight zero even though it has in-degree (out-degree) greater than zero
\end{itemize}
\item Stability
\begin{itemize}
\item Measures the effect on rankings if the graph is perturbed: edges are added or deleted
\item HITS is insensitive to minor changes
\item If a set of pages in V' have low PageRank, then the change in PageRank will be small
\end{itemize}
\end{itemize}
\section{Hubs and Authorities}
\begin{itemize}
\item Two primary kinds of web pages useful as results for broad-topic searches - informational query
\begin{itemize}
\item Authorities - pages with sources of information on a certain topic
\item Hubs - not in themselves sources of information, but compilations to authoritative pages
\end{itemize}
\item Circular definition - iterative computation
\end{itemize}
\section{Choosing the subset of the Web}
\begin{itemize}
\item Good authority pages may not contain the specific query term
\item Following procedure suggested: 
\begin{enumerate}
\item Given a query (e.g. leukaemia) use a text index to get all pages containing leukaemia, call this the \textit{root set} of pages
\item Build a \textit{base set} of pages, to include the root set as well as any page that either links to the a page in the root set, or is linked to by a page in the root set
\end{enumerate}
\item Then use the base set for computing hub and authority scores
\item Base set constructed in this way because:
\begin{enumerate}
\item A good authority page may not contain the query text
\item If the text query manages to capture a good hub page $v_h$ in the root set, then the inclusion of all pages inked to by any page in the root set will capture all the good authorities linked to by $v_h$ in the base set
\item Conversely, if the text query manages to capture a good authority page $v_a$ in the root set, then the inclusion of pages points to $v_a$ will bring other good hubs into the base set. In other words, the 'expansion' of the root set into the base set enriches the common pool of good hubs and authorities
\end{enumerate}
\item Suffices to use around 200 pages for the root set, rather than all pages matching the text query
\item A small number of iterations of the power method can compute a relative scoring of hubs and authorities
\item Shown around 5 iterations yield fairly good results
\end{itemize}
\section{Improved algorithms for topic distillation}
\begin{itemize}
\item Connectivity analysis been shown to be useful in identifying high quality pages within a topic specific graph
\item Users tend to have short queries
\item Find documents relevant to topic of search
\item Finding quality documents on a query topic - topic distillation
\item If A is seen to point to a lot of good documents, then A becomes more valuable, adn the fact that A points to B would suggest that B is a good document as well
\item Link to KELINBERG - developed a connectivity analysis algorithm for hyperlinked environments
\item Three probelms with connectivity analysis
\begin{enumerate}
\item Mutually reinforcing relationships between hosts - set of documents on one post point to single document on second host, cand drive up hub scores on first host, and authority on second - undue weight to the opinion of one person
\item Automatically generated links - Links do not represent human opinion
\item Non-relevant documents - Topic drift, most highly ranked authorities and hubs tend to not be about original topic
\end{enumerate} 
\item General solutions - use content analysis
\item Hub and aucthority scores done as follows;
\begin{enumerate}
\item Let \textit{N} be the set of nodes in the neighbourhood graph
\item For every node \textit{n} in \textit{N}, let \textit{H[n]} be its hub score and \textit{A[n]} its authority score.
\item Initialize \textit{H[n]} and \textit{A[n]} to 1 for all \textit{n} in \textit{N}.
\item While the vectors \textit{H} and \textit{A} have not converged:
\item For all \textit{n} in \textit{N}, $A[n]:=\sum_{(n',n)\in N} H[n']$
\item For all \textit{n} in \textit{N}, $H[n]:=\sum_{(n,n')\in N} A[n']$
\item Normalize the \textit{H} and \textit{A} vectors
\end{enumerate}
\item in practice, found that the vectors converge in about 10 iterations
\item Improves precision over Kleinberg by 45\%
\item Limited to topics that are well represented and well connected on the Web
\item Assumes that results of a SE define a good start set
\item Topic drift can be avoided by using topic vecotrs to filter the expanded subgraph - topic vector is a term vector computer from the text of pages in the seed set, a page is allowed to remain in the extended graph only if its term vector is a good match for this topic vector
\end{itemize}
\section{Authoritative sources in a hyperlinked environment}
\begin{itemize}
\item Specific queries, broad-topic queries, similar-page queries
\item Scarcity problem - not many pages have the exact info you desire, and often difficult to identify these pages
\item Abundance problem - number of pages that could be returned as relevant is too much for a human user to digest
\item Need to distil a broad topic down to a representation of very small size
\end{itemize}
\section{PageRank, HITS, and a unified framework for Link analysis}
\begin{itemize}
\item HITS emphasises mutual reinforcement between authority and hub web pages
\item PageRank emphasizes hyper link weight normalisation and web surfing based on random walk models 
\end{itemize}
\section{Link Analysis, eigenvectors and stability}
\begin{itemize}
\item It is worth noting that HITS is typically described as running on a small collection of articles (say retrieved in response to a query), while PageRank is described in terms of the entire web
\end{itemize}
\section{The stochastic approach for link-structure analysis (SALSA) and the TKC effect}
\begin{itemize}
\item Examines random walks on graphs derived from link structure
\item Computationally more efficient that the mutual reinforcement
\item TKC effect - Tightly Knitted Community
\begin{itemize}
\item TKC effect can derail the mutual reinforcement approach
\item Occurs when a community scores high in link analysing algorithms but have no authority on the topic, or just pertain to one aspect of the topic
\end{itemize}
\item Examines walks on two different Markov Chains which are derived from the link structure of the WWW, Authority chain and hub chain
\item Authorities correspond to the sites that are most frequently visited by the random walk defined by the the authority Markov Chain
\item SALSA less vulnerable to the TKC effect
\end{itemize}
\section{Authority rankings from HITS, PageRank and SALSA}
\begin{itemize}
\item HITS and SALSA eigenvectors need not be unique, PageRank does
\item SALSA can give inconsistent hub and authority weights due to non-uniqueness
\item HITS and SALSA can behave unexpectedly on certain graphs - output can depend on initial seed vector, output of HITS can depend on whether the implementation uses initial authority or hub vector
\item Authority and hub vectors produced by SALSA can be inconsistent
\item PageRank doesn't display this behaviour
\end{itemize}
\section{Modelling the internet and the web}
\subsection{HITS}
\begin{itemize}
\item Computes a pair of scoring values associated with hypertext documents
\item Subgraph of interest is computed by selecting the neighbours of a root set of web pages relevant to the topic
\item Children of a given node are forward links and can be obtained directly from v
\item Parents of a node correspond to backlinks
\item Bounding number of parents is crucial in practical applications
\item The authority weight of a page is the sum of the hubness weights of its parents
\item The hubness weight of a page is the sum of the authority weights of its children
\item \textbf{BHARAT and HERZINGER} suggested improved version of HITS, modified by assigning weight to these multiple edges that are inversely proportional to their multiplicity
\end{itemize}
%--------------------------------------------------------------------
\chapter{APPLICATIONS}
\section{PageRank beyond web}
\begin{itemize}
\item Biology - GeneRank
\item PageRank used as a network centrality measure - help understand graph better by focusing on what PageRank reveals as important
\item Used to illuminate a region of a large graph around a target set of interest - localized measure, personalized PageRank
\item Reverse PageRank - follow inlinks instead of outlinks
\end{itemize}
\subsection{Chemistry}
\begin{itemize}
\item Valence of a molecule is the number of potential bonds it can make
\item PageRank to study molecules in chemistry - Mooney et al 2002
\item Use PageRank to assess the change in a network of molecules linked by hydrogen bonds among water molecules
\item Graph contains edges between water molecules if they have a potential hydrogen bond to a solute molecule
\item Goal to assess the hydrogen bond potential of a solvent
\item REFERENCE: \textbf{B. L. Mooney, L. R. Corrales, and A. E. Clark}, \textit{Molecularnetworks: An integrated graph
theoretic and data mining tool to explore solvent organization in molecular simulation.}
\end{itemize}
\subsection{Neuroscience}
\begin{itemize}
\item Evaluate the importance of brain regions given observed correlations of brain activity
\end{itemize}
\subsection{Roads and Urban spaces}
\begin{itemize}
\item Predict both traffic flow and human movement
\item Natural road - continuous path built from road segments joining adjacent segments together
\item PageRank with $\alpha=0.95$ , Jiang et al, PageRank best network measure in terms of predicting traffic on individual roads - 15000 nodes and 50000 edges
\item REFERENCE:\textbf{B. Jiang, S. Zhao, and J. Yin}, \textit{Self-organized natural roads for predicting traffic flow: a sensitivity
study}
\item Urban spaces - largest space of a city observable from a single vantage point - Jiang 2009, urban space best considered as a city neighbourhood or block
\item Urban space network connects adjacent spaces, or blocks, if physically adjacent
\item Urban spaces in London have up to 20000 nodes and 100000 links
\item Weighted PageRank best predicts human mobility in London - $\alpha=1$ accounts for up to 60\% of movement
\item Individuals cannot teleport
\item REFERENCE:\textbf{B. Jiang}, \textit{Ranking spaces for predicting human movement in an urban environment}
\end{itemize}
\subsection{Sports}
\begin{itemize}
\item Winner network
\item Idea underlying these rankings is that of a random fan that follows a team until another team beats them, at which point they pick up the new team, and periodically restarts with an arbitrary team
\end{itemize}
\subsection{ItemRank}
\begin{itemize}
\item Netflix and Amazon's recommender systems - item recommendation
\item Users rate items, and wish to recommend items with high ratings
\item Ratings matrix is a items-by-users matrix where $R_{ij}$ is the numeric value given to item $i$ by user $j$
\item Form bipartite graph
\item G weighted graph where weights on an edge $(i,j)$ are the number of users that rated both items $i$ and $j$
\item \textbf{P} be the standard weighted random walk construction on G
\item Then ItemRank scores are solutions of \begin{equation}
(\textbf{I} -\alpha\textbf{P}) \textbf{S}= (1-\alpha)\textbf{RD}_\textbf{R}^{\textbf{-1}}
\end{equation} 
where $\textbf{D}_\textbf{R}$ are the column sums of the rating matrix
\item Each column of \textbf{S} is a set of recommendations for user \textit{j}
\item REFERENCE:\textbf{M. Gori and A. Pucci}, \textit{ItemRank: a random-walk based scoring algorithm for recommender engines}
\end{itemize}
\subsection{Wikipedia}
\begin{itemize}
\item Use PageRank to generate reading lists automatically Preparation of topical reading lists from the link structure of Wikipedia
\item List of most important people \textbf{Y.-H. Eom, P. AragonÂ´ , D. Laniado, A. Kaltenbrunner, S. Vigna, and D. L. Shepelyansky} \textit{Interactions of cultures and top people of Wikipedia from ranking of 24 language editions}
\end{itemize}
\section{Chemistry}
\subsection{MoleculaRnetworks: An integrated graph theoretic and data mining tool to explore solvent organization in molecular simulation}
\begin{itemize}
\item Novel method for identifying geometric shape adopted by the solvent in the immediate vicinity of the solute, and an exploratory approach for describing H-bonding - based on PageRank algorithm
\item Designed primarily for systems where the solvent is water and a single solute is available
\item Used to discern effects of the solute on the H-bond network
\item Analyses of network structure, including clustering of solvent molecules and connectivity information
\item Instantaneously identify the geometric organization of the solvent about the solute, so dynamics of solvent shells can be measured
\item PageRank of each vertex (water molecule) is determined by the number of edges (H-bonds) connected to it, as well as the number of edges connected to its neighbours.
\item PageRank used to assess the H-Bond connectivity of the solvent
\begin{figure}
\includegraphics[width=\linewidth]{Decomposition_of_simulation_date_into_a_graph_-chem.jpg}
\caption{Decomposition of simulation data into a graph}
\label{fig:chem}
\end{figure}
\item Code with $\alpha$ set at 0.85, also can be 0.99
\item PageRank peaks influenced by number of neighbour waters
\item Lower $\alpha$ increases the influence of the neighbours
\item Each uniquely connected polyhedron has its own unique vector representation, and so able to access database for confirmation of polyhedra
\item Enhances the traditional, often non standardized methods used to extract chemical information from molecular simulation
\end{itemize}
\section{Roads and Urban Spaces}
\subsection{doi:10.1080/13658810802022822}
\begin{itemize}
\item City can be topologically represented as a connectivity graph, consisting of nodes representing individual spaces and links if the corresponding spaces are intersected
\item Can capture human movement rates in individual spaces
\item Weighted PageRank
\item PageRank scores are significantly correlated to human movement both pedestrian and vehicle in four areas of London
\begin{figure}
\includegraphics[width=\linewidth]{map_view.jpeg}
\caption{(a) A fictive urban system, (b) its axial map and (c) connectivity graph Source as above}
\label{fig:map}
\end{figure}
\item When applied to ranking space for an urban environment, connected and undirected graph, so no dangling nodes involved
\item Weighted PageRank score are significantly correlated to the pedestrian and vehicle flows
\item Correlation gets better as $\alpha$ increases
\item Random walker never gets stuck, and always moves to one of its successors with non-uniform probability, well-connected streets are more favourable
\item over 60\% of human movement can be predicted or explained purely from a topological point of view. In terms of statistics, the other 40\% is not predictable, and it may relate to land use, building height and road width
\item Uses a never-get-stuck-or-tired random walker
\end{itemize}
\subsection{1742-5468-2008-07-P07008}
\begin{itemize}
\item Weighted PageRank ($\alpha= 0.95$) is still the best metric (R square over 0.7) in terms of metricâ€“flow correlation
\item In addition, we found that weighted PageRank with an
appropriate d factor setting tends to be one of the best metrics for correlating or predicting
traffic flow
\end{itemize}
\section{Recommending Systems}
\subsection{ItemRank: A Random-Walk Based Scoring Algorithm for Recommender Engines}
\begin{itemize}
\item Emerging technology to help consumers find interesting products
\item Makes personalized product suggestions by extracting knowledge from the previous users interactions
\item ItemRank - random-walk bases scoring algorithm used to rank products according to expected user preferences
\item ItemRank performs better than other algorithms and also less complex
\item For each user, know the ratings they've assigned
\item ItemRank - biased PageRank
\item Efficient - need around 20 iterations
\end{itemize}
\section{Wikipedia}
\subsection{Preparation of Topical Reading Lists from the Link Structure of Wikipedia}
\begin{itemize}
\item Personalized background reading lists can be generated automatically from the link structure of Wikipedia
\item Used combination localized and global PageRank
\end{itemize}
\subsection{Interactions of cultures and top people of Wikipedia from ranking of 24 language editions}
\begin{itemize}
\item Rank articles to find top 100 historical figures
\item Finds global historical figures
\item Find most influential cultures
\item Most of the top historical figures in Wikipedia were born in Western countries
\item Strong male skew
\item 'The reason for a somewhat unexpected PageRank leader Carl Linnaeus is related to the fact that he laid the foundations for the modern biological naming scheme so that plenty of articles about animals, insects and plants point to the Wikipedia article about him, which strongly increases the PageRank probability.'
\item Culture has high PageRank if has many inlinks from other cultures
\item  'we find for all centuries at the top positions Greek, Turkish and Arabic'
\item 'historical figures before
the 19th century, we find respectively Arabic, Turkish and Greek'
\item 'most important historical figures across Wikipedia language editions were born in Western countries after the 17th century, and are male'
\item mathematical analysis of local and global historical figures can be a useful
step towards the understanding of local and global history and interactions of world cultures
\end{itemize}
\end{document}